{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the file\n",
    "test_source_path = './dataset/MPBD/test.en'\n",
    "test_target_path = './dataset/MPBD/test.sen'\n",
    "\n",
    "with open(test_source_path, 'r', encoding='utf-8') as file:\n",
    "    test_source_lines = file.readlines()\n",
    "with open(test_target_path, 'r', encoding='utf-8') as file:\n",
    "    test_target_lines = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting results from LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pathlib\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from google.colab import userdata\n",
    "from vertexai.generative_models import GenerationConfig\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_output(prompt, llm):\n",
    "    if llm == 3.5:\n",
    "        openai.api_key = ''\n",
    "        model = 'gpt-3.5-turbo-1106'\n",
    "        message = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        )\n",
    "        result = message['choices'][0]['message']['content']\n",
    "\n",
    "    elif llm == 4:\n",
    "        openai.api_key = ''\n",
    "        model = 'gpt-4'\n",
    "        message = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        )\n",
    "        result = message['choices'][0]['message']['content']\n",
    "\n",
    "    elif llm == 'instruct':\n",
    "        openai.api_key = ''\n",
    "        model = \"gpt-3.5-turbo-instruct\"\n",
    "        message = openai.Completion.create(\n",
    "            model = model,\n",
    "            prompt = prompt,\n",
    "            temperature = 0    \n",
    "        )\n",
    "        result = message['choices'][0]['text']   \n",
    "    \n",
    "    elif llm == 'gemini-1.0-pro':\n",
    "        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "        safety_settings = [\n",
    "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"}\n",
    "        ]        \n",
    "\n",
    "        response = model.generate_content(prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                candidate_count=1,\n",
    "                temperature=0),\n",
    "                safety_settings=safety_settings)\n",
    "    \n",
    "        result = response.candidates[0].content.parts[0].text\n",
    "    \n",
    "\n",
    "    elif llm == 'flan-ul2':\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", device_map=\"auto\", load_in_8bit=True)                                                                 \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(inputs, max_length=200)\n",
    "        result = tokenizer.decode(outputs[0])\n",
    "    \n",
    "\n",
    "    elif llm == 'med-alpaca':\n",
    "        pipeline = pipeline(\"text-generation\", model=\"medalpaca/medalpaca-7b\", tokenizer=\"medalpaca/medalpaca-7b\")\n",
    "        result = pipeline(prompt, max_length=200)[0]['generated_text']\n",
    "\n",
    "    elif llm == 'pmc-llama':\n",
    "        tokenizer = LlamaTokenizer.from_pretrained('axiong/PMC_LLaMA_13B')\n",
    "        model = LlamaForCausalLM.from_pretrained('axiong/PMC_LLaMA_13B')\n",
    "        encoded_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        output = model.generate(**encoded_input)\n",
    "        result = tokenizer.decode(output[0])\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Invalid LLM')\n",
    "\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(input_text, prompt_type='base'):\n",
    "    \n",
    "    # Initial part of the prompt that describes the task\n",
    "    prompt = '''### Task\n",
    "You are a skilled medical expert. Simplify the given medical text, making complex medical information accessible and relevant to people without a medical background.\n",
    "'''\n",
    "    \n",
    "    if prompt_type == 'guide':\n",
    "        prompt += f'''\n",
    "### Summarization Guide\n",
    "Remove unnecessary details: Identify and omit details that may not contribute to a layperson's understanding of the key findings. This could include experimental setups, control experiments, and specific quantitative results.\n",
    "Provide relevant background information: Offer essential background information such as the prevalence, mortality, risk factors, and outcomes of the condition being discussed. \n",
    "Simplify medical jargon: Whenever possible, replace medical terms and jargon with common language or provide clear, concise definitions for technical terms. Minimize the use of abbreviations to ensure clarity.\n",
    "Sentence structure simplification: Break down complex sentences into shorter, more digestible segments. \n",
    "'''\n",
    "        \n",
    "    elif prompt_type == '1shot':\n",
    "        prompt += f'''\n",
    "### Examples\n",
    "Example Input 1: in the present study we sought to critically test whether earlier findings on the positive effect of sensorimotor rhythm neurofeedback on sleep quality and memory could also be replicated in a double-blind placebo-controlled study on 25 patients with insomnia.\n",
    "Example Output 1: in the study researchers sought to test whether earlier findings on the positive effect of neurofeedback on sleep quality and memory could also be replicated in a double-blind placebo-controlled study.\n",
    "'''  \n",
    "\n",
    "    elif prompt_type == '3shot':\n",
    "        prompt += f'''\n",
    "### Examples\n",
    "Example Input 1: in the present study we sought to critically test whether earlier findings on the positive effect of sensorimotor rhythm neurofeedback on sleep quality and memory could also be replicated in a double-blind placebo-controlled study on 25 patients with insomnia.\n",
    "Example Output 1: in the study researchers sought to test whether earlier findings on the positive effect of neurofeedback on sleep quality and memory could also be replicated in a double-blind placebo-controlled study.\n",
    "\n",
    "Example Input 2: exposure to unilateral oophorectomy is associated with a 7-year earlier age at menopause.\n",
    "Example Output 2: having undergone unilateral oophorectomy is also associated with a 7-year earlier age at menopause.\n",
    "\n",
    "Example Input 3: we genotyped 43 596 individuals from the danish general population followed between january 1976 and july 2007.\n",
    "Example Output 3: the population-based study genotyped 43 596 danish people followed between january 1976 and july 2007.\n",
    "'''   \n",
    "            \n",
    "    elif prompt_type == '5shot':\n",
    "        prompt += f'''\n",
    "### Examples\n",
    "Example Input 1: in the present study we sought to critically test whether earlier findings on the positive effect of sensorimotor rhythm neurofeedback on sleep quality and memory could also be replicated in a double-blind placebo-controlled study on 25 patients with insomnia.\n",
    "Example Output 1: in the study researchers sought to test whether earlier findings on the positive effect of neurofeedback on sleep quality and memory could also be replicated in a double-blind placebo-controlled study.\n",
    "\n",
    "Example Input 2: exposure to unilateral oophorectomy is associated with a 7-year earlier age at menopause.\n",
    "Example Output 2: having undergone unilateral oophorectomy is also associated with a 7-year earlier age at menopause.\n",
    "\n",
    "Example Input 3: we genotyped 43 596 individuals from the danish general population followed between january 1976 and july 2007.\n",
    "Example Output 3: the population-based study genotyped 43 596 danish people followed between january 1976 and july 2007.\n",
    "\n",
    "Example Input 4: few studies have evaluated the combined impact of following recommended lifestyle behaviors on cancer, cardiovascular disease and all-cause mortality, and most included tobacco avoidance.\n",
    "Example Output 4: few studies have evaluated the combined impact of following recommended lifestyle behaviors on cancer, cardiovascular disease, and all-cause mortality, and most of those included tobacco avoidance as one of the recommendations.\n",
    "\n",
    "Example Input 5: of 920 patients who underwent chest radiography, 110 had abnormal findings consistent with pneumonia.\n",
    "Example Output 5: of 920 patients who had a chest x-ray, a minority had abnormal findings consistent with pneumonia.\n",
    "'''      \n",
    "\n",
    "    # Add input text to be annotated\n",
    "    prompt += f'''\n",
    "### Input Text: {input_text}\n",
    "### Output Text:\n",
    "'''\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(llm, prompt_type, test_source_lines):\n",
    "    output_path = f'./output/MPBD/{llm}/'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Check if the file exists and delete it before starting to append new outputs\n",
    "    output_file_path = os.path.join(output_path, f'test_{prompt_type}.predict')\n",
    "    if os.path.exists(output_file_path):\n",
    "        os.remove(output_file_path)\n",
    "    \n",
    "    for source_line in test_source_lines: \n",
    "        success = False\n",
    "        while not success:\n",
    "            try:\n",
    "                # Assume get_output is a function that generates the output\n",
    "                prompt = create_prompt(source_line, prompt_type)\n",
    "                output = get_output(prompt, llm)\n",
    "                cleaned_output = ' '.join(output.split())\n",
    "                \n",
    "                # Open the file in append mode ('a') to add each new output\n",
    "                with open(output_file_path, 'a', encoding='utf-8') as f_write:\n",
    "                    f_write.write(cleaned_output + '\\n')\n",
    "                    success = True\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = 4\n",
    "prompt_type = 'guide'\n",
    "run(llm, prompt_type, test_source_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (overlapping with golden labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results from specified dir\n",
    "llm = 'ul2'\n",
    "prompt_type = 'base'\n",
    "output_file_path = f'./output/MPBD/{llm}/test_{prompt_type}.predict'\n",
    "\n",
    "with open(output_file_path, 'r', encoding='utf-8') as file:\n",
    "    test_predict_lines = file.readlines()\n",
    "\n",
    "# Pairing target with predict lines\n",
    "paired_lines = []\n",
    "for target_line, predict_line in zip(test_target_lines, test_predict_lines):\n",
    "    paired_lines.append([target_line.strip(), predict_line.strip()])\n",
    "    #print(len(target_line), len(predict_line))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paired_lines[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# `paired_lines` is list of [gold_standard, predicted_output] pairs\n",
    "# For example: paired_lines = [['The cat sits on the mat.', 'A cat is on a mat.']]\n",
    "\n",
    "# Initialize the Rouge scoring object\n",
    "rouge = Rouge()\n",
    "\n",
    "# Prepare lists to hold scores for each metric\n",
    "scores_rouge1 = []\n",
    "scores_rouge2 = []\n",
    "scores_rougel = []\n",
    "\n",
    "# Iterate over each pair and calculate ROUGE scores\n",
    "for gold_standard, predicted_output in paired_lines:\n",
    "    # Calculate scores\n",
    "    scores = rouge.get_scores(predicted_output, gold_standard, avg=False)\n",
    "    \n",
    "    # Append scores for each metric\n",
    "    scores_rouge1.append(scores[0]['rouge-1']['f'])\n",
    "    scores_rouge2.append(scores[0]['rouge-2']['f'])\n",
    "    scores_rougel.append(scores[0]['rouge-l']['f'])\n",
    "\n",
    "# Calculate average scores\n",
    "avg_rouge1 = sum(scores_rouge1) / len(scores_rouge1)\n",
    "avg_rouge2 = sum(scores_rouge2) / len(scores_rouge2)\n",
    "avg_rougel = sum(scores_rougel) / len(scores_rougel)\n",
    "\n",
    "print(f\"Average ROUGE-1 Score: {avg_rouge1}\")\n",
    "print(f\"Average ROUGE-2 Score: {avg_rouge2}\")\n",
    "print(f\"Average ROUGE-L Score: {avg_rougel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score_list = []\n",
    "meteor_sc_list = []\n",
    "\n",
    "for label_text, predicted_text in paired_lines:\n",
    "    # Calculate BLEU score\n",
    "    reference = [label_text.split()]\n",
    "    candidate = predicted_text.split()\n",
    "    bleu_score = sentence_bleu(reference, candidate, weights=[0.5, 0.5])\n",
    "\n",
    "    # Tokenize the predicted text\n",
    "    tokenized_predicted_text = word_tokenize(predicted_text)\n",
    "    tokenized_label_text = word_tokenize(label_text)\n",
    "\n",
    "    # Calculate METEOR score\n",
    "    meteor_sc = meteor_score([tokenized_label_text], tokenized_predicted_text)\n",
    "\n",
    "    bleu_score_list.append(bleu_score)\n",
    "    meteor_sc_list.append(meteor_sc)\n",
    "\n",
    "\n",
    "print(f\"Average BLEU Score: {np.mean(np.array(bleu_score_list))}\")\n",
    "print(f\"Average METEOR Score: {np.mean(np.array(meteor_sc_list))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "# Prepare lists to hold scores for each metric\n",
    "scores_flesch_kincaid_grade = []\n",
    "scores_gunning_fog = []\n",
    "scores_coleman_liau_index = []\n",
    "\n",
    "# Iterate over each pair and calculate ROUGE scores\n",
    "for gold_standard, predicted_output in paired_lines:\n",
    "    flesch_kincaid_grade = textstat.flesch_kincaid_grade(predicted_output)\n",
    "    gunning_fog = textstat.gunning_fog(predicted_output)\n",
    "    coleman_liau_index = textstat.coleman_liau_index(predicted_output)\n",
    "    \n",
    "    # Append scores for each metric\n",
    "    scores_flesch_kincaid_grade.append(flesch_kincaid_grade)\n",
    "    scores_gunning_fog.append(gunning_fog)\n",
    "    scores_coleman_liau_index.append(coleman_liau_index)\n",
    "\n",
    "# Calculate average scores\n",
    "avg_flesch_kincaid_grade = sum(scores_flesch_kincaid_grade) / len(scores_flesch_kincaid_grade)\n",
    "avg_gunning_fog = sum(scores_gunning_fog) / len(scores_gunning_fog)\n",
    "avg_coleman_liau_index = sum(scores_coleman_liau_index) / len(scores_coleman_liau_index)\n",
    "\n",
    "print(f\"Average flesch_kincaid_grade: {avg_flesch_kincaid_grade}\")\n",
    "print(f\"Average gunning_fog: {avg_gunning_fog}\")\n",
    "print(f\"Average coleman_liau_index: {avg_coleman_liau_index}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
