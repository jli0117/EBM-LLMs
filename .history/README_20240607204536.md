# Benchmarking Large Language Models in Evidence-Based Medicine

Evidence-based medicine (EBM) represents a paradigm of providing patient care grounded in the most current and rigorously evaluated research. Recent advances in large language models (LLMs) offer a potential solution to transform EBM by automating labor-intensive tasks and thereby improving the efficiency of clinical decision-making.
This study explores integrating LLMs into the key stages in EBM, evaluating their ability across evidence retrieval (PICO extraction, biomedical question answering), synthesis (summarizing randomized controlled trials), and dissemination (medical text simplification). We conducted a comparative analysis of seven LLMs, including both proprietary and open-source models, as well as those fine-tuned on medical corpora. Specifically, we benchmarked the performance of various LLMs on each EBM task under zero-shot settings as baselines, and employed prompting techniques, including in-context learning, chain-of-thought reasoning, and knowledge-guided prompting to enhance their capabilities.

## Detailed descriptions

Four tasks in evidence-based medicine is benchmarked in our code, incluing PICO extraction, biomedical question answering (QA), summarizing randomized controlled trials (RCT), and medical text simplification.

### PICO extraction

The dataset for PICO extraction can be downloaded from: https://github.com/BIDS-Xu-Lab/section_specific_annotation_of_PICO/tree/main/data/EBM-NLPmod

### Biomedical QA

You can modify the hyperparameters for training in the `params` dictionary within the main function. Expand `params_list` to include multiple hyperparameter configurations for grid search.

The dataset for biomedical QA can be downloaded from ...

### RCT summarizaiton 

The dataset for PICO extraction can be downloaded from ...

The training process involves:

1. Reading and normalizing the dataset.
2. Defining time steps for prediction.
3. Reshaping the data into sequences suitable for RNN input.
4. Training the RGAN-LS model with specified hyperparameters.

### Medical text simplification

The dataset for PICO extraction can be downloaded from ...

The training process involves:

1. Reading and normalizing the dataset.
2. Defining time steps for prediction.
3. Reshaping the data into sequences suitable for RNN input.
4. Training the RGAN-LS model with specified hyperparameters.


## License

This project is licensed under the Apache License 2.0 - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* https://github.com/BIDS-Xu-Lab/section_specific_annotation_of_PICO
* https://github.com/BIDS-Xu-Lab/Clinical_Entity_Recognition_Using_GPT_models