{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random, os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "dataset = './dataset/PICO/test/'\n",
    "search_pattern = os.path.join(dataset, '*.bio')  # Use os.path.join for compatibility\n",
    "files = glob.glob(search_pattern)\n",
    "\n",
    "# Normalize path to always use forward slashes\n",
    "files = [file.replace(os.sep, '/') for file in files]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting results from GPTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "def get_output(prompt, GPT):\n",
    "    if GPT == 3.5:\n",
    "        openai.api_key = ''\n",
    "        model = 'gpt-3.5-turbo-1106'\n",
    "        message = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        )\n",
    "        result = message['choices'][0]['message']['content']\n",
    "\n",
    "    elif GPT == 4:\n",
    "        openai.api_key = ''\n",
    "        model = 'gpt-4'\n",
    "        message = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        )\n",
    "        result = message['choices'][0]['message']['content']\n",
    "\n",
    "    elif GPT == 'instruct':\n",
    "        openai.api_key = ''\n",
    "        model = \"gpt-3.5-turbo-instruct\"\n",
    "        message = openai.Completion.create(\n",
    "            model = model,\n",
    "            prompt = prompt,\n",
    "            temperature = 0    \n",
    "        )\n",
    "        result = message['choices'][0]['text']   \n",
    "\n",
    "\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(input_text, prompt_type='base'):\n",
    "    \n",
    "    # Initial part of the prompt that describes the task\n",
    "    prompt = '''### Task\n",
    "You are a skilled medical expert. Your task is to generate an HTML version of an input text, marking up specific entities related to healthcare. The entities to be identified are: 'Participant', 'Intervention', 'Control', and 'Outcomes'. Use HTML <span> tags to highlight these entities. Each <span> should have a class attribute indicating the type of the entity.\n",
    "\n",
    "### Markup Format\n",
    "Use <span class=\"participant\"> to denote a participant entity.\n",
    "Use <span class=\"intervention\"> to denote a intervention entity.\n",
    "Use <span class=\"control\"> to denote a control entity.\n",
    "Use <span class=\"outcome\"> to denote a outcome entity.\n",
    "Leave the text as it is if no such entities are found.\n",
    "'''\n",
    "\n",
    "    if prompt_type == 'guide':\n",
    "        prompt += f'''\n",
    "### Entity Recognition Guide\n",
    "'Participant' refers to the descriptions of participants involved in a medical study, including details about their recruitment process and the characteristics or requirements they needed to meet for inclusion. These descriptions typically encompass various relevant factors such as age, gender, sample size, medical diagnoses or conditions, treatment locations, and other specific details pertinent to the study being conducted. These population descriptors provide important context and help define the target group under investigation.\n",
    "'Intervention' refers to the proposed treatment or approach being administered to the participants. While interventions commonly refer to medical treatments in the medical literature, it's important to note that interventions can also encompass non-medical approaches, such as educational courses or musical therapies. The intervention is the specific action or method being implemented with the aim of addressing or influencing the condition or problem being studied.\n",
    "'Control' refers to the comparison or control treatment utilized in many studies. The control group serves as a baseline for comparison to evaluate the effectiveness of the intervention in terms of the desired outcomes. In some cases, the control group may receive a placebo treatment, which is an inactive substance or sham procedure that mimics the appearance of the actual intervention but lacks the active components. Alternatively, the control group may receive no treatment at all. These control treatments are implemented to provide a reference point for assessing the impact of the intervention and determining its efficacy in relation to the desired outcomes.\n",
    "'Outcome' refers to the measurements or observations used to assess the effectiveness of the treatment in individuals participating in a trial. Outcomes are often described by specifying the specific score, scale, measurement tool, or clinical test utilized to evaluate the desired outcome. In clinical trials, researchers compare outcomes between two or more groups of patients, each receiving a different treatment. These outcomes serve as measurable indicators to determine whether the treatment has produced the intended effect or achieved the desired result. By comparing outcomes across different treatment groups, researchers can assess the comparative effectiveness of the interventions being studied.\n",
    "'''\n",
    "    # few-shot learning\n",
    "    elif prompt_type == '1shot':\n",
    "        prompt += f'''\n",
    "### Examples\n",
    "Example Input 1: Acupuncture with sham device twice a week for six weeks or placebo pill once a day for eight weeks .\n",
    "Analysis 1: In this example, 'Acupuncture with sham device' is an intervention entity, 'placebo pill' is a control entity.\n",
    "'''\n",
    "    elif prompt_type == '3shot':\n",
    "        prompt += f'''\n",
    "Example Input 1: Acupuncture with sham device twice a week for six weeks or placebo pill once a day for eight weeks .\n",
    "Analysis 1: In this example, 'Acupuncture with sham device' is an intervention entity, 'placebo pill' is a control entity.\n",
    "\n",
    "Example Input 2: Comparison of participants who remained on placebo continued beyond the run - in period to the end of the study .\n",
    "Analysis 2: In this example, 'placebo' is a control entity.\n",
    "\n",
    "Example Input 3: Arm pain measured on a 10 point pain .\n",
    "Analysis 3: In this example, 'Arm pain' and 'a 10 point pain' are outcome entities.\n",
    "'''\n",
    "    elif prompt_type == '5shot':\n",
    "        prompt += f'''\n",
    "### Examples\n",
    "Example Input 1: Acupuncture with sham device twice a week for six weeks or placebo pill once a day for eight weeks .\n",
    "Analysis 1: In this example, 'Acupuncture with sham device' is an intervention entity, 'placebo pill' is a control entity.\n",
    "\n",
    "Example Input 2: Comparison of participants who remained on placebo continued beyond the run - in period to the end of the study .\n",
    "Analysis 2: In this example, 'placebo' is a control entity.\n",
    "\n",
    "Example Input 3: Arm pain measured on a 10 point pain .\n",
    "Analysis 3: In this example, 'Arm pain' and 'a 10 point pain' are outcome entities.\n",
    "\n",
    "Example Input 4: Plerixafor plus granulocyte colony versus placebo plus granulocyte colony\n",
    "Analysis 4: In this example, 'Plerixafor plus granulocyte colony' is a intervention entity, 'placebo plus granulocyte colony' is a control entity\n",
    "\n",
    "Example Input 5: Effect of coenzyme Q10 in patients with hormonally untreated carcinoma of the prostate\n",
    "Analysis 5: In this example, 'coenzyme Q10' is a intervention entity, 'patients with hormonally untreated carcinoma of the prostate' is a participant entity\n",
    "'''\n",
    "\n",
    "    # Add input text to be annotated\n",
    "    prompt += f'''\n",
    "### Input Text: {input_text}\n",
    "### Output Text:\n",
    "'''\n",
    "    #print(prompt)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(GPT, prompt_type):\n",
    "    for i, file in enumerate(files):\n",
    "        with open(file,'r') as f_read:\n",
    "            text = ' '.join([line.split('\\t')[0] for line in f_read.read().splitlines()])\n",
    "        file_name = file.split('/')[-1].split('.')[0]\n",
    "\n",
    "        dir_path = f'./output/PICO/{GPT}/{prompt_type}/'\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "        success = False\n",
    "        while not success:\n",
    "            try:\n",
    "                prompt = create_prompt(text, prompt_type)\n",
    "                output = get_output(prompt, GPT)\n",
    "                with open(f'./output/PICO/{GPT}/{prompt_type}/{file_name}.html','w') as f_write:\n",
    "                    f_write.write(output)\n",
    "                    success = True\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT = 4\n",
    "prompt_type = 'base'\n",
    "run(GPT, prompt_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import NavigableString, Tag\n",
    "import spacy\n",
    "\n",
    "py_nlp = spacy.load (\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html2bio(html_path):\n",
    "    with open(html_path) as f:\n",
    "        \n",
    "        html = f.read()\n",
    "        \n",
    "        if '***output***' in html.lower():\n",
    "            html = html[html.lower().index('***output***')+len('***output***')+1:]\n",
    "        if 'output:' in html.lower():\n",
    "            html = html[html.lower().index('output:')+len('output:')+1:]\n",
    "        if 'output text' in html.lower():\n",
    "            html = html[html.lower().index('output text')+len('output text')+1:]\n",
    "        if '***Highlighted Text***'  in html.lower():\n",
    "            html = html[html.lower().index('***Highlighted Text***')+len('***Highlighted Text***')+1:]\n",
    "        if '<body>' in html:\n",
    "            html = html[html.index('<body>')+6:html.index('</body>')]\n",
    "        if '<p>' in html:\n",
    "            html = html[html.index('<p>')+3:html.index('</p>')]\n",
    "            \n",
    "        #print (html_path)\n",
    "        #print (html,'\\n')\n",
    "        \n",
    "        # Parse HTML using BeautifulSoup\n",
    "        soup = bs(html, \"html.parser\")\n",
    "\n",
    "        # Extract text under 'p' tags and convert to BIO format\n",
    "        bio_format = []\n",
    "        \n",
    "\n",
    "        for child in soup.children:\n",
    "            if isinstance(child, NavigableString):\n",
    "                for word in child.split():\n",
    "                    bio_format.append(f\"{word}\\tO\\n\")  ### split each word, and append -> 'of\\tO\\n' (O - outside)\n",
    "            elif isinstance(child, Tag):\n",
    "                words = py_nlp (child.get_text())\n",
    "                try:\n",
    "                    entity = child.attrs['class'][0]\n",
    "                except:\n",
    "                    entity = 'O'\n",
    "                if len(words) != 0:\n",
    "                    if entity != 'O' and entity in ['participant', 'intervention', 'control', 'outcome']:\n",
    "                        ## first token: B - beginning\n",
    "                        if entity == 'participant':\n",
    "                            bio_format.append(f\"{words[0]}\\tB-P\\n\") \n",
    "                        elif entity == 'intervention':\n",
    "                            bio_format.append(f\"{words[0]}\\tB-I\\n\") \n",
    "                        elif entity == 'control':\n",
    "                            bio_format.append(f\"{words[0]}\\tB-C\\n\") \n",
    "                        elif entity == 'outcome':\n",
    "                            bio_format.append(f\"{words[0]}\\tB-O\\n\")\n",
    "                        ## second to last: I - inside\n",
    "                        for word in words[1:]:\n",
    "                            if entity == 'participant':\n",
    "                                bio_format.append(f\"{word}\\tI-P\\n\") \n",
    "                            elif entity == 'intervention':\n",
    "                                bio_format.append(f\"{word}\\tI-I\\n\") \n",
    "                            elif entity == 'control':\n",
    "                                bio_format.append(f\"{word}\\tI-C\\n\") \n",
    "                            elif entity == 'outcome':\n",
    "                                bio_format.append(f\"{word}\\tI-O\\n\")    \n",
    "                    else:\n",
    "                        bio_format.append(f\"{words[0]}\\tO\\n\")\n",
    "                        for word in words[1:]:\n",
    "                            bio_format.append(f\"{word}\\tO\\n\")\n",
    "    return bio_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_sub_sentences(output_text):\n",
    "    # Define the regular expression pattern to match each entity type individually\n",
    "    pattern = r'(Participant|Intervention|Control|Outcome)\\s+entities\\s+are\\s*(.*?)(?=(Participant|Intervention|Control|Outcome)\\s+entities\\s+are|\\Z)'\n",
    "\n",
    "    # Find all matches using the pattern\n",
    "    matches = re.findall(pattern, output_text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Extract sub-sentences for each entity type\n",
    "    sub_sentences = [''] * 4\n",
    "    for match in matches:\n",
    "        entity_type = match[0].strip().lower()\n",
    "        sub_sentence = match[1].strip().rstrip(',')  # Remove trailing comma\n",
    "        if entity_type == 'participant':\n",
    "            sub_sentences[0] = sub_sentence\n",
    "        elif entity_type == 'intervention':\n",
    "            sub_sentences[1] = sub_sentence\n",
    "        elif entity_type == 'control':\n",
    "            sub_sentences[2] = sub_sentence\n",
    "        elif entity_type == 'outcome':\n",
    "            sub_sentences[3] = sub_sentence\n",
    "\n",
    "    return sub_sentences\n",
    "\n",
    "\n",
    "def txt2bio(output_path, ori_tokens):\n",
    "        \n",
    "    # read the result file\n",
    "    with open(output_path, 'r') as file:\n",
    "        output_text = file.read()\n",
    "\n",
    "        # print(ori_tokens)\n",
    "        # print(output_text) \n",
    "\n",
    "        # if none, than make all o\n",
    "        if output_text.lower() == 'none':\n",
    "            entity_list = ['O' for _ in ori_tokens]\n",
    "        else: \n",
    "            entity_types = ['P', 'I', 'C', 'O']\n",
    "            sub_sentences = split_sub_sentences(output_text)\n",
    "\n",
    "            # Initialize a dictionary to store the extracted entity types\n",
    "            extracted_entities = {}\n",
    "\n",
    "            # Use regex to extract entities for each sub-sentence\n",
    "            for entity_type, sub_sentence in zip(entity_types, sub_sentences):\n",
    "                entities = re.split(r', |\\. ', sub_sentence)\n",
    "                entities = [entity.rstrip('.') for entity in entities]\n",
    "                extracted_entities[entity_type] = entities\n",
    "                \n",
    "            # Print the extracted entities\n",
    "            ##print(extracted_entities)\n",
    "\n",
    "            # Initialize entity_list with 'O' for all tokens\n",
    "            entity_list = ['O' for _ in ori_tokens]\n",
    "\n",
    "            # Iterate over the tokens and check if they belong to any extracted entity\n",
    "            for entity_type, entities in extracted_entities.items():\n",
    "                for entity in entities:\n",
    "                    if entity:  # Check if the entity is not empty\n",
    "                        # Check if the entity is present in the original tokens as a whole phrase\n",
    "                        entity_length = len(entity.split())\n",
    "                        occurrences = [i for i in range(len(ori_tokens)) if ' '.join(ori_tokens[i:i+entity_length]) == entity]\n",
    "                        for start_index in occurrences:\n",
    "                            if entity_list[start_index] == 'O':\n",
    "                                # Annotate the tokens belonging to the entity\n",
    "                                entity_list[start_index] = f\"B-{entity_type}\"\n",
    "                                for j in range(start_index + 1, start_index + entity_length):\n",
    "                                    entity_list[j] = f\"I-{entity_type}\"\n",
    "\n",
    "        # make sure the results look similar to bio_format above\n",
    "        bio_format = [f\"{token}\\t{entity}\\n\" for token, entity in zip(ori_tokens, entity_list)]\n",
    "        ##print(bio_format)\n",
    "    \n",
    "    return bio_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(GPT, prompt_type, html=True):\n",
    "    all_tags = []    # predicted labels\n",
    "    all_tokens = []  # all tokens\n",
    "    gold_tags = []   # golden labels\n",
    "\n",
    "    for file in files:  # files are the golden standard target\n",
    "        file_name = file.split('/')[-1].split('.')[0]\n",
    "        with open(file) as f_gold:\n",
    "            lines = f_gold.readlines()\n",
    "            tokens = [line.strip().split('\\t')[0] for line in lines]  # tokens: e.g., ['ABC', 'is', 'a', '70', ...]\n",
    "            tags = [line.strip().split('\\t')[-1] for line in lines]   # golden: e.g., ['o',   'o',  'o', 'I', ...]\n",
    "\n",
    "            if html: \n",
    "                prediction = f'./output/PICO/GPT-{GPT}/{prompt_type}/{file_name}.html'\n",
    "                bio_2 = html2bio(prediction)  # e.g., ['Renal\\tB-problem\\n', 'cell\\tI-problem\\n', 'carcinoma\\tI-problem\\n', 'is\\tO\\n', ...]\n",
    "            else:\n",
    "                prediction = f'./output/PICO/GPT-{GPT}/{prompt_type}/{file_name}.output'\n",
    "                bio_2 = txt2bio(prediction, tokens)\n",
    "\n",
    "            all_tokens += tokens\n",
    "\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token != '':\n",
    "                    match = False\n",
    "                    for i2 in range(i,-1,-1):\n",
    "                        try:\n",
    "                            # extract token and the corresponding predicted labels, e.g., ['Renal', 'B-problem']\n",
    "                            token_2, tag_2 = bio_2[i2].strip().split('\\t') \n",
    "                        except:\n",
    "                            token_2, tag_2 = None, None\n",
    "                        \n",
    "                        if token_2!=None:\n",
    "                            if token in token_2 or token_2 in token:\n",
    "                                match = True\n",
    "                                break\n",
    "\n",
    "                    if not match:\n",
    "                        tag_2 = 'O'\n",
    "                else:\n",
    "                    tag_2 = ''\n",
    "                \n",
    "                gold_tags.append(tags[i])\n",
    "                all_tags.append(tag_2)\n",
    "\n",
    "            ##print(gold_tags, all_tags)\n",
    "                \n",
    "    with open(f'./output/PICO/GPT-{GPT}/{prompt_type}/merged_gold_pred.bio','w') as fg:\n",
    "        for i, (token, gold_tag, all_tag) in enumerate(zip(all_tokens, gold_tags, all_tags)):\n",
    "            if token!='':\n",
    "                fg.write(f'{token}\\t{gold_tag}\\t{all_tag}\\n')\n",
    "            else:\n",
    "                fg.write(f'\\n')\n",
    "    !python ./evaluate_pico.py -lf ./output/PICO/GPT-{GPT}/{prompt_type}/merged_gold_pred.bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results from specified dir\n",
    "GPT = 'ul2'\n",
    "prompt_type = 'guide'\n",
    "\n",
    "# get performance\n",
    "get_performance(GPT, prompt_type, False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
