{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = './dataset/PubMedQA/test.json'\n",
    "\n",
    "# Initialize lists to store the individual fields\n",
    "test_id_list = []\n",
    "test_question_list = []\n",
    "test_context_list = []\n",
    "test_longlabel_list = []\n",
    "test_shortlabel_list = []\n",
    "\n",
    "# Open the JSON file and read line by line\n",
    "with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Parse each line (which is a JSON object) into a Python dictionary\n",
    "        sample = json.loads(line.strip())\n",
    "        \n",
    "        # Extract and store each field in its respective list\n",
    "        test_id_list.append(sample.get('pubid'))\n",
    "        test_question_list.append(sample.get('question'))\n",
    "        list_of_context = sample.get('context')['contexts']\n",
    "        test_context_list.append(\" \".join(list_of_context))\n",
    "        test_longlabel_list.append(sample.get('long_answer'))\n",
    "        test_shortlabel_list.append(sample.get('final_decision'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting results from LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pathlib\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from google.colab import userdata\n",
    "from vertexai.generative_models import GenerationConfig\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_output(prompt, llm):\n",
    "    if llm == 3.5:\n",
    "        openai.api_key = ''\n",
    "        model = 'gpt-3.5-turbo-1106'\n",
    "        message = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        )\n",
    "        result = message['choices'][0]['message']['content']\n",
    "\n",
    "    elif llm == 4:\n",
    "        openai.api_key = ''\n",
    "        model = 'gpt-4'\n",
    "        message = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        )\n",
    "        result = message['choices'][0]['message']['content']\n",
    "\n",
    "    elif llm == 'instruct':\n",
    "        openai.api_key = ''\n",
    "        model = \"gpt-3.5-turbo-instruct\"\n",
    "        message = openai.Completion.create(\n",
    "            model = model,\n",
    "            prompt = prompt,\n",
    "            temperature = 0    \n",
    "        )\n",
    "        result = message['choices'][0]['text']   \n",
    "    \n",
    "    elif llm == 'gemini-1.0-pro':\n",
    "        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "        safety_settings = [\n",
    "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"}\n",
    "        ]        \n",
    "\n",
    "        response = model.generate_content(prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                candidate_count=1,\n",
    "                temperature=0),\n",
    "                safety_settings=safety_settings)\n",
    "    \n",
    "        result = response.candidates[0].content.parts[0].text\n",
    "    \n",
    "\n",
    "    elif llm == 'flan-ul2':\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", device_map=\"auto\", load_in_8bit=True)                                                                 \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(inputs, max_length=200)\n",
    "        result = tokenizer.decode(outputs[0])\n",
    "    \n",
    "\n",
    "    elif llm == 'med-alpaca':\n",
    "        pipeline = pipeline(\"text-generation\", model=\"medalpaca/medalpaca-7b\", tokenizer=\"medalpaca/medalpaca-7b\")\n",
    "        result = pipeline(prompt, max_length=200)[0]['generated_text']\n",
    "\n",
    "    elif llm == 'pmc-llama':\n",
    "        tokenizer = LlamaTokenizer.from_pretrained('axiong/PMC_LLaMA_13B')\n",
    "        model = LlamaForCausalLM.from_pretrained('axiong/PMC_LLaMA_13B')\n",
    "        encoded_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        output = model.generate(**encoded_input)\n",
    "        result = tokenizer.decode(output[0])\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Invalid LLM')\n",
    "\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question, reference, prompt_type='base'):\n",
    "    \n",
    "    if prompt_type == 'guide':\n",
    "        prompt = f'''### Task\n",
    "You are a skilled medical expert. Considering the information from a biomedical study provided in the reference text, is it correct to conclude that '{question}'? \n",
    "Please begin with a brief response of 'Yes', 'No' or 'Maybe'. Then, provide a detailed explanation in 1-2 sentences.\n",
    "Finally, briefly outline your reasoning process by stating: 'Reasoning Process: ', ensuring it aligns with the findings of the study.\n",
    "'''\n",
    "    else:\n",
    "        prompt = f'''### Task\n",
    "You are a skilled medical expert. Considering the information from a biomedical study provided in the reference text, is it correct to conclude that '{question}'? \n",
    "Please begin with a brief response of 'Yes', 'No' or 'Maybe'. Then, provide a detailed explanation in 1-2 sentences.\n",
    "'''\n",
    "\n",
    "\n",
    "    if prompt_type == '1shot':\n",
    "        prompt += f''' \n",
    "### Examples\n",
    "Example Question 1: Visceral adipose tissue area measurement at a single level: can it represent visceral adipose tissue volume?\n",
    "Example Reference Text 1: Measurement of visceral adipose tissue (VAT) needs to be accurate and sensitive to change for risk monitoring. The purpose of this study is to determine the CT slice location where VAT area can best reflect changes in VAT volume and body weight.\", \"60 plain abdominal CT images from 30 males [mean age (range) 51 (41-68) years, mean body weight (range) 71.1 (101.9-50.9) kg] who underwent workplace screenings twice within a 1-year interval were evaluated. Automatically calculated and manually corrected areas of the VAT of various scan levels using \\\"freeform curve\\\" region of interest on CT were recorded and compared with body weight changes.\", \"The strongest correlations of VAT area with VAT volume and body weight changes were shown in a slice 3 cm above the lower margin of L3 with r values of 0.853 and 0.902, respectively.\n",
    "Example Output Text 1: Yes. VAT area measurement at a single level 3 cm above the lower margin of the L3 vertebra is feasible and can reflect changes in VAT volume and body weight. Advances in knowledge: As VAT area at a CT slice 3cm above the lower margin of L3 can best reflect interval changes in VAT volume and body weight, VAT area measurement should be selected at this location.\n",
    "'''\n",
    "\n",
    "    elif prompt_type == '3shot':\n",
    "        prompt += f''' \n",
    "### Examples\n",
    "Example Question 1: Visceral adipose tissue area measurement at a single level: can it represent visceral adipose tissue volume?\n",
    "Example Reference Text 1: Measurement of visceral adipose tissue (VAT) needs to be accurate and sensitive to change for risk monitoring. The purpose of this study is to determine the CT slice location where VAT area can best reflect changes in VAT volume and body weight.\", \"60 plain abdominal CT images from 30 males [mean age (range) 51 (41-68) years, mean body weight (range) 71.1 (101.9-50.9) kg] who underwent workplace screenings twice within a 1-year interval were evaluated. Automatically calculated and manually corrected areas of the VAT of various scan levels using \\\"freeform curve\\\" region of interest on CT were recorded and compared with body weight changes.\", \"The strongest correlations of VAT area with VAT volume and body weight changes were shown in a slice 3 cm above the lower margin of L3 with r values of 0.853 and 0.902, respectively.\n",
    "Example Output Text 1: Yes. VAT area measurement at a single level 3 cm above the lower margin of the L3 vertebra is feasible and can reflect changes in VAT volume and body weight. Advances in knowledge: As VAT area at a CT slice 3cm above the lower margin of L3 can best reflect interval changes in VAT volume and body weight, VAT area measurement should be selected at this location.\n",
    "\n",
    "Example Question 2: Can a practicing surgeon detect early lymphedema reliably?\n",
    "Example Reference Text 2: Lymphedema may be identified by simpler circumference changes as compared with changes in limb volume.\", \"Ninety breast cancer patients were prospectively enrolled in an academic trial, and seven upper extremity circumferences were measured quarterly for 3 years. A 10% volume increase or greater than 1 cm increase in arm circumference identified lymphedema with verification by a lymphedema specialist. Sensitivity and specificity of several different criteria for detecting lymphedema were compared using the academic trial as the standard.\", \"Thirty-nine cases of lymphedema were identified by the academic trial. Using a 10% increase in circumference at two sites as the criterion, half the lymphedema cases were detected (sensitivity 37%). When using a 10% increase in circumference at any site, 74.4% of cases were detected (sensitivity 49%). Detection by a 5% increase in circumference at any site was 91% sensitive.\n",
    "Example Output Text 2: Maybe. An increase of 5% in circumference measurements identified the most potential lymphedema cases compared with an academic trial.\n",
    "\n",
    "Example Question 3: It's Fournier's gangrene still dangerous?\n",
    "Example Reference Text 3: Fournier's gangrene is known to have an impact in the morbidity and despite antibiotics and aggressive debridement, the mortality rate remains high.\", \"To assess the morbidity and mortality in the treatment of Fournier's gangrene in our experience.\", \"The medical records of 14 patients with Fournier's gangrene who presented at the University Hospital Center \\\"Mother Teresa\\\" from January 1997 to December 2006 were reviewed retrospectively to analyze the outcome and identify the risk factor and prognostic indicators of mortality.\", \"Of the 14 patients, 5 died and 9 survived. Mean age was 54 years (range from 41-61): it was 53 years in the group of survivors and 62 years in deceased group. There was a significant difference in leukocyte count between patients who survived (range 4900-17000/mm) and those died (range 20.300-31000/mm3). Mean hospital stay was about 19 days (range 2-57 days).\n",
    "Example Output Text 3: Yes. The interval from the onset of clinical symptoms to the initial surgical intervention seems to be the most important prognostic factor with a significant impact on outcome. Despite extensive therapeutic efforts, Fournier's gangrene remains a surgical emergency and early recognition with prompt radical debridement is the mainstays of management.\n",
    "'''\n",
    "\n",
    "    elif prompt_type == '5shot':\n",
    "        prompt += f''' \n",
    "### Examples\n",
    "Example Question 1: Visceral adipose tissue area measurement at a single level: can it represent visceral adipose tissue volume?\n",
    "Example Reference Text 1: Measurement of visceral adipose tissue (VAT) needs to be accurate and sensitive to change for risk monitoring. The purpose of this study is to determine the CT slice location where VAT area can best reflect changes in VAT volume and body weight.\", \"60 plain abdominal CT images from 30 males [mean age (range) 51 (41-68) years, mean body weight (range) 71.1 (101.9-50.9) kg] who underwent workplace screenings twice within a 1-year interval were evaluated. Automatically calculated and manually corrected areas of the VAT of various scan levels using \\\"freeform curve\\\" region of interest on CT were recorded and compared with body weight changes.\", \"The strongest correlations of VAT area with VAT volume and body weight changes were shown in a slice 3 cm above the lower margin of L3 with r values of 0.853 and 0.902, respectively.\n",
    "Example Output Text 1: Yes. VAT area measurement at a single level 3 cm above the lower margin of the L3 vertebra is feasible and can reflect changes in VAT volume and body weight. Advances in knowledge: As VAT area at a CT slice 3cm above the lower margin of L3 can best reflect interval changes in VAT volume and body weight, VAT area measurement should be selected at this location.\n",
    "\n",
    "Example Question 2: Can a practicing surgeon detect early lymphedema reliably?\n",
    "Example Reference Text 2: Lymphedema may be identified by simpler circumference changes as compared with changes in limb volume.\", \"Ninety breast cancer patients were prospectively enrolled in an academic trial, and seven upper extremity circumferences were measured quarterly for 3 years. A 10% volume increase or greater than 1 cm increase in arm circumference identified lymphedema with verification by a lymphedema specialist. Sensitivity and specificity of several different criteria for detecting lymphedema were compared using the academic trial as the standard.\", \"Thirty-nine cases of lymphedema were identified by the academic trial. Using a 10% increase in circumference at two sites as the criterion, half the lymphedema cases were detected (sensitivity 37%). When using a 10% increase in circumference at any site, 74.4% of cases were detected (sensitivity 49%). Detection by a 5% increase in circumference at any site was 91% sensitive.\n",
    "Example Output Text 2: Maybe. An increase of 5% in circumference measurements identified the most potential lymphedema cases compared with an academic trial.\n",
    "\n",
    "Example Question 3: It's Fournier's gangrene still dangerous?\n",
    "Example Reference Text 3: Fournier's gangrene is known to have an impact in the morbidity and despite antibiotics and aggressive debridement, the mortality rate remains high.\", \"To assess the morbidity and mortality in the treatment of Fournier's gangrene in our experience.\", \"The medical records of 14 patients with Fournier's gangrene who presented at the University Hospital Center \\\"Mother Teresa\\\" from January 1997 to December 2006 were reviewed retrospectively to analyze the outcome and identify the risk factor and prognostic indicators of mortality.\", \"Of the 14 patients, 5 died and 9 survived. Mean age was 54 years (range from 41-61): it was 53 years in the group of survivors and 62 years in deceased group. There was a significant difference in leukocyte count between patients who survived (range 4900-17000/mm) and those died (range 20.300-31000/mm3). Mean hospital stay was about 19 days (range 2-57 days).\n",
    "Example Output Text 3: Yes. The interval from the onset of clinical symptoms to the initial surgical intervention seems to be the most important prognostic factor with a significant impact on outcome. Despite extensive therapeutic efforts, Fournier's gangrene remains a surgical emergency and early recognition with prompt radical debridement is the mainstays of management.\n",
    "\n",
    "Example Question 4: Does a colonoscopy after acute diverticulitis affect its management?\n",
    "Example Reference Text 4: Medical records of 220 patients hospitalized for acute diverticulitis between June 1, 2002 and September 1, 2009 were reviewed. Acute diverticulitis was diagnosed by clinical criteria and characteristic CT findings. Fifteen patients were excluded either because of questionable CT or hematochezia. Mean age was 61.8±14.3 years (61% females). Clinical parameters, laboratory results, imaging, endoscopic and histopathological reports, and long-term patients' outcome were analyzed.\", \"One hundred patients (aged 61.8±13.3 y, 54.1% females), underwent an early (4 to 6 wk) colonoscopy after hospital discharge. There were no significant differences in patients' characteristics or survival between those with or without colonoscopy (4±1.9 vs. 4.2±2.1 y, P=0.62). No colonic malignancy was detected. However, in 32 patients (32%) at least 1 polyp was found. Only 1 was determined as an advanced adenoma. No new or different diagnosis was made after colonoscopy.\n",
    "Example Output Text 4: No. Our results suggest that colonoscopy does not affect the management of patients with acute diverticulitis nor alter the outcome. The current practice of a routine colonoscopy after acute diverticulitis, diagnosed by typical clinical symptoms and CT needs to be reevaluated.\n",
    "\n",
    "Example Question 5: Detailed analysis of sputum and systemic inflammation in asthma phenotypes: are paucigranulocytic asthmatics really non-inflammatory?\n",
    "Example Reference Text 5: The technique of induced sputum has allowed to subdivide asthma patients into inflammatory phenotypes according to their level of granulocyte airway infiltration. There are very few studies which looked at detailed sputum and blood cell counts in a large cohort of asthmatics divided into inflammatory phenotypes. The purpose of this study was to analyze sputum cell counts, blood leukocytes and systemic inflammatory markers in these phenotypes, and investigate how those groups compared with healthy subjects.\", \"We conducted a retrospective cross-sectional study on 833 asthmatics recruited from the University Asthma Clinic of Liege and compared them with 194 healthy subjects. Asthmatics were classified into inflammatory phenotypes.\", \"The total non-squamous cell count per gram of sputum was greater in mixed granulocytic and neutrophilic phenotypes as compared to eosinophilic, paucigranulocytic asthma and healthy subjects (p < 0.005). Sputum eosinophils (in absolute values and percentages) were increased in all asthma phenotypes including paucigranulocytic asthma, compared to healthy subjects (p < 0.005). Eosinophilic asthma showed higher absolute sputum neutrophil and lymphocyte counts than healthy subjects (p < 0.005), while neutrophilic asthmatics had a particularly low number of sputum macrophages and epithelial cells. All asthma phenotypes showed an increased blood leukocyte count compared to healthy subjects (p < 0.005), with paucigranulocytic asthmatics having also increased absolute blood eosinophils compared to healthy subjects (p < 0.005). Neutrophilic asthma had raised CRP and fibrinogen while eosinophilic asthma only showed raised fibrinogen compared to healthy subjects (p < 0.005).\n",
    "Example Output Text 5: Maybe. This study demonstrates that a significant eosinophilic inflammation is present across all categories of asthma, and that paucigranulocytic asthma may be seen as a low grade inflammatory disease.\n",
    "'''\n",
    "\n",
    "    # Add input text to be annotated\n",
    "    prompt += f'''\n",
    "### Reference Text: {reference}\n",
    "### Output Text:\n",
    "'''\n",
    "\n",
    "    #print(prompt)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(llm, prompt_type, ids, questions, contexts):\n",
    "    output_path = f'./output/PubMedQA/{llm}/'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Check if the file exists and delete it before starting to append new outputs\n",
    "    output_file_path = os.path.join(output_path, f'test_{prompt_type}.json')\n",
    "    if os.path.exists(output_file_path):\n",
    "        os.remove(output_file_path)\n",
    "    \n",
    "    for id, question, context in zip(ids, questions, contexts): \n",
    "        success = False\n",
    "        while not success:\n",
    "            try:\n",
    "                # Assume get_output is a function that generates the output\n",
    "                prompt = create_prompt(question, context, prompt_type)\n",
    "                output = get_output(prompt, llm)\n",
    "\n",
    "                # todo: model may not generate proper answer??\n",
    "                cleaned_output = ' '.join(output.split())\n",
    "\n",
    "                # Create a json string\n",
    "                output_dict = {\"id\": id, \"answer\": cleaned_output}\n",
    "                json_string = json.dumps(output_dict)\n",
    "                \n",
    "                # Open the file in append mode ('a') to add each new output\n",
    "                with open(output_file_path, 'a', encoding='utf-8') as f_write:\n",
    "                    f_write.write(json_string + '\\n')\n",
    "                    success = True\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = 4\n",
    "prompt_type = 'base'\n",
    "run(llm, prompt_type, test_id_list, test_question_list, test_context_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Load results from the specified directory\n",
    "output_file_path = f'./output/PubMedQA/{llm}/test_{prompt_type}.json'\n",
    "\n",
    "test_id2_list = []\n",
    "test_short_answer_list = []\n",
    "test_long_answer_list = []\n",
    "\n",
    "# Open the JSON file and read line by line\n",
    "with open(output_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        sample = json.loads(line.strip())\n",
    "\n",
    "        # Extract sample id\n",
    "        sample_id = sample.get('id')\n",
    "        test_id2_list.append(sample_id)\n",
    "\n",
    "        # Extract the answer\n",
    "        sample_answer = sample.get('answer')\n",
    "\n",
    "        if prompt_type == 'guide':\n",
    "            # If under CoT prompt, we only keep the long answer before \"Reasoning Process\"\n",
    "            short_answer_match = re.search(r\"^\\s*(Yes|No|Maybe)\", sample_answer, re.IGNORECASE)\n",
    "            short_answer = short_answer_match.group(0) if short_answer_match else \"Answer not found!\"\n",
    "            \n",
    "            long_answer_match = re.search(r\"^\\s*(Yes|No|Maybe)[\\s,]*(.*?)(?=\\s*Reasoning Process:|$)\", sample_answer, re.IGNORECASE | re.DOTALL)\n",
    "            long_answer = long_answer_match.group(2).strip() if long_answer_match else \"Long answer not found!\"\n",
    "        \n",
    "        else:\n",
    "            short_answer_match = re.search(r\"^\\s*(Yes|No|Maybe)\", sample_answer, re.IGNORECASE)\n",
    "            short_answer = short_answer_match.group(0) if short_answer_match else \"Answer not found!\"\n",
    "            \n",
    "            long_answer_match = re.search(r\"^\\s*(Yes|No|Maybe)[\\s,]*(.*)\", sample_answer, re.IGNORECASE | re.DOTALL)\n",
    "            long_answer = long_answer_match.group(2).strip() if long_answer_match else \"Long answer not found!\"\n",
    "\n",
    "        # Append the extracted answers to their respective lists\n",
    "        test_short_answer_list.append(short_answer)\n",
    "        test_long_answer_list.append(long_answer)\n",
    "\n",
    "# Ensure that the IDs match between the original and extracted lists\n",
    "assert test_id_list == test_id2_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy for short answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize  # Added label_binarize import\n",
    "\n",
    "print(\"Evaluating: \", llm, prompt_type)\n",
    "\n",
    "# Define possible options for labels\n",
    "valid_labels = ['yes', 'no', 'maybe']\n",
    "\n",
    "# Convert labels and predictions to lowercase and filter only valid options\n",
    "labels = [label.lower() for label in test_shortlabel_list if label.lower() in valid_labels]\n",
    "predictions = [prediction.lower() for prediction in test_short_answer_list if prediction.lower() in valid_labels]\n",
    "\n",
    "# Ensure both lists are of the same length after filtering\n",
    "if len(labels) != len(predictions):\n",
    "    raise ValueError(\"Mismatch between the number of valid labels and predictions.\")\n",
    "\n",
    "# Convert labels to binary values using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(valid_labels)\n",
    "labels_binary = le.transform(labels)\n",
    "predictions_binary = le.transform(predictions)\n",
    "\n",
    "# Binarize the labels and predictions\n",
    "classes = [0, 1, 2] \n",
    "labels_binarized = label_binarize(labels_binary, classes=classes)\n",
    "predictions_binarized = label_binarize(predictions_binary, classes=classes)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels_binary, predictions_binary)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "try:\n",
    "    # **Modification: Calculate AUC using the binarized labels and predictions**\n",
    "    auc_score = roc_auc_score(labels_binarized, predictions_binarized, average='macro', multi_class='ovr')\n",
    "    print(\"AUC Score (One-vs-Rest):\", auc_score)\n",
    "except ValueError as e:\n",
    "    print(f\"Unable to calculate AUC: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rouge score for long answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# Initialize the Rouge scoring object\n",
    "rouge = Rouge()\n",
    "\n",
    "# Prepare lists to hold scores for each metric\n",
    "scores_rouge1 = []\n",
    "scores_rouge2 = []\n",
    "scores_rougel = []\n",
    "\n",
    "# Iterate over each pair and calculate ROUGE scores\n",
    "for idx, gold_standard, predicted_output in zip(test_id_list, test_longlabel_list, test_long_answer_list):\n",
    "\n",
    "    try:\n",
    "        # Calculate scores\n",
    "        scores = rouge.get_scores(predicted_output, gold_standard, avg=False)\n",
    "        \n",
    "        # Append scores for each metric\n",
    "        scores_rouge1.append(scores[0]['rouge-1']['f'])\n",
    "        scores_rouge2.append(scores[0]['rouge-2']['f'])\n",
    "        scores_rougel.append(scores[0]['rouge-l']['f'])\n",
    "        \n",
    "    except ValueError as e:\n",
    "        # Check if the error is due to an empty hypothesis\n",
    "        if \"Hypothesis is empty.\" in str(e):\n",
    "            print(\"Hypothesis is empty in id:\", idx)\n",
    "\n",
    "# Calculate average scores\n",
    "avg_rouge1 = sum(scores_rouge1) / len(scores_rouge1)\n",
    "avg_rouge2 = sum(scores_rouge2) / len(scores_rouge2)\n",
    "avg_rougel = sum(scores_rougel) / len(scores_rougel)\n",
    "\n",
    "print(f\"Average ROUGE-1 Score: {avg_rouge1}\")\n",
    "print(f\"Average ROUGE-2 Score: {avg_rouge2}\")\n",
    "print(f\"Average ROUGE-L Score: {avg_rougel}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
